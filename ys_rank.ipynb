{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ページデータ取得\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://shopping.yahoo.co.jp/categoryranking/42794/list?sc_i=shp_pc_catelist_rankingTab'\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "filename =  'page.txt'\n",
    "with open(filename, mode='w') as f:\n",
    "    f.write(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力サイトの上位10位データ取得とExcel登録\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "\n",
    "url = input('Yahoo!Shoppingの商品ランキングページURLを貼り付けて下さい')\n",
    "if url == '':\n",
    "    url = 'https://shopping.yahoo.co.jp/categoryranking/42794/list?sc_i=shp_pc_catelist_rankingTab'\n",
    "    \n",
    "page = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "categories = page.find_all('a', class_ = 'Breadcrumb__link')\n",
    "l_cat = []\n",
    "for category in categories:\n",
    "    l_cat.append(category.text)\n",
    "\n",
    "# print(l_cat)\n",
    "# print()\n",
    "\n",
    "ranks = page.find_all('span', class_ = 'rank-text')\n",
    "reviews = page.find_all('span', class_ = 'Review__count____2_0_88 Review__count--hasBrackets____2_0_88 review-count')\n",
    "averages = page.find_all('span', class_ = 'Review__average____2_0_88 review-average')\n",
    "prices = page.find_all('span', class_ = 'price-number')\n",
    "names = page.find_all('span', class_ = 'name-text')\n",
    "images = page.find_all('img', class_ = 'image')\n",
    "\n",
    "# for (rank, review, average, price, name, image) in zip(ranks, reviews, averages, prices, names, images):\n",
    "#     print(rank.text, review.text, average.text, price.text, name.text)\n",
    "#     print(image.get('src'))\n",
    "\n",
    "# print(len(ranks), len(reviews), len(averages), len(prices), len(names), len(images))\n",
    "\n",
    "folder = Path('output')\n",
    "folder.mkdir(exist_ok=True)\n",
    "excel_path = folder.joinpath('SpreadSheet10.xlsx')\n",
    "\n",
    "wb = openpyxl.load_workbook(excel_path)\n",
    "ws = wb['Ranking']\n",
    "\n",
    "for j in range(len(l_cat)):\n",
    "    ws.cell(row=6, column=j + 2, value=l_cat[j])\n",
    "\n",
    "print(len(reviews))\n",
    "for i in range(len(ranks)):\n",
    "    ws.cell(row=i + 9, column=3, value=reviews[i].text)\n",
    "    ws.cell(row=i + 9, column=4, value=averages[i].text)\n",
    "    ws.cell(row=i + 9, column=5, value=prices[i].text)\n",
    "    ws.cell(row=i + 9, column=6, value=names[i].text)\n",
    "    img_url = images[i].get('src')\n",
    "    ws.cell(row=i + 9, column=7, value=img_url)\n",
    "    img_data = requests.get(img_url)\n",
    "    img_path = folder.joinpath(img_url.split('/')[-1] + '.jpg')\n",
    "    with open(img_path, mode='wb') as f:\n",
    "        f.write(img_data.content)\n",
    "\n",
    "wb.save(excel_path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas経由でcategoryとRankingシートを作成\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "excel_path = folder.joinpath('category.xlsx')\n",
    "sr = pd.Series(l_cat, name='カテゴリー')\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    sr.to_excel(writer, sheet_name='category', index=False)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for (rank, review, average, price, name, image) in zip(ranks, reviews, averages, prices, names, images):\n",
    "    row = {'順位':rank.text, 'レビュー数':review.text, 'アベレージ':average.text, '単価':price.text, '商品名':name.text, '画像':image.get('src')}\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=False)\n",
    "    df_i = df.set_index('順位')\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a') as writer:\n",
    "    df_i.to_excel(writer, sheet_name='ranking', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in ./.venv/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./.venv/lib/python3.11/site-packages (from selenium) (2023.11.17)\n",
      "Collecting attrs>=20.1.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/461.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, sniffio, pysocks, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.1.0 h11-0.14.0 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Googleで'Selenium'を検索し、「Seleniumブラウザー自動化プロジェクト」をクリック\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--user-data-dir=/Users/ynurmj5e/Library/Application Support/Google/Chrome/Default')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "# Googleの検索TOP画面を開く。\n",
    "driver.get(\"https://www.google.co.jp/\")\n",
    "# 検索語として「selenium」と入力し、Enterキーを押す。\n",
    "driver.find_element(By.NAME, \"q\").send_keys(\"selenium\")\n",
    "driver.find_element(By.NAME, \"q\").send_keys(Keys.ENTER)\n",
    "# タイトルに「Seleniumブラウザー自動化プロジェクト」をクリックする。\n",
    "driver.find_element(By.CLASS_NAME, \"VuuXrf\").click()\n",
    "# 5秒間待機する。\n",
    "sleep(5)\n",
    "# ブラウザを終了する。\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルサーバーテストサイトの「もっと見る」を２度クリック→「閉じる」のテキストをprintしてクリック\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--user-data-dir=/Users/ynurmj5e/Library/Application Support/Google/Chrome/Default')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"http://10.211.55.7\")\n",
    "sleep(1)\n",
    "\n",
    "driver.find_element(By.CLASS_NAME, \"js-btn-more\").click()\n",
    "sleep(1)\n",
    "\n",
    "driver.find_element(By.CLASS_NAME, \"js-btn-more\").click()\n",
    "sleep(1)\n",
    "\n",
    "btn = driver.find_element(By.CLASS_NAME, \"js-btn-close\")\n",
    "print(btn.text)\n",
    "btn.click()\n",
    "sleep(1)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel記載サイトの『もっと見る」をクリックし、上位30位データを表示・取得してExcel登録\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "folder = Path('output')\n",
    "folder.mkdir(exist_ok=True)\n",
    "\n",
    "excel_path = folder.joinpath('SpreadSheet30.xlsx')\n",
    "wb = openpyxl.load_workbook(excel_path)\n",
    "ws = wb['Ranking']\n",
    "url = ws['C2'].value\n",
    "\n",
    "page = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "categories = page.find_all('li', class_ = 'Breadcrumb__item')\n",
    "\n",
    "l_cat = []\n",
    "for category in categories:\n",
    "    l_cat.append(category.text)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument('--user-data-dir=/Users/ynurmj5e/Library/Application Support/Google/Chrome/Default')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "sleep(1)\n",
    "driver.execute_script(\"window.scrollTo(0, 2000);\")\n",
    "sleep(1)\n",
    "more_btn = driver.find_element(By.CLASS_NAME, \"button-text\").click()\n",
    "sleep(1)\n",
    "\n",
    "page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# ranks = page.find_all('span', class_ = 'rank-text')\n",
    "reviews = page.find_all('span', class_ = 'Review__count____2_0_88 Review__count--hasBrackets____2_0_88 review-count')\n",
    "averages = page.find_all('span', class_ = 'Review__average____2_0_88 review-average')\n",
    "prices = page.find_all('span', class_ = 'price-number')\n",
    "names = page.find_all('span', class_ = 'name-text')\n",
    "images = page.find_all('img', class_ = 'image')\n",
    "\n",
    "# for (rank, review, average, price, name, image) in zip(ranks, reviews, averages, prices, names, images):\n",
    "#     print(rank.text, review.text, average.text, price.text, name.text)\n",
    "#     print(image.get('src'))\n",
    "\n",
    "for j in range(len(l_cat)):\n",
    "    ws.cell(row=6, column=j + 2, value=l_cat[j])\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    ws.cell(row=i + 9, column=3, value=reviews[i].text)\n",
    "    ws.cell(row=i + 9, column=4, value=averages[i].text)\n",
    "    ws.cell(row=i + 9, column=5, value=prices[i].text)\n",
    "    ws.cell(row=i + 9, column=6, value=names[i].text)\n",
    "    img_url = images[i].get('src')\n",
    "    ws.cell(row=i + 9, column=7, value=img_url)\n",
    "    img_data = requests.get(img_url)\n",
    "    img_path = folder.joinpath(img_url.split('/')[-1] + '.jpg')\n",
    "    \n",
    "    with open(img_path, mode='wb') as f:\n",
    "        f.write(img_data.content)\n",
    "\n",
    "wb.save(excel_path)\n",
    "wb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.0\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.17.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in ./.venv/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./.venv/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting attrs>=20.1.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, typing_extensions, sniffio, pysocks, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.2.0 h11-0.14.0 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.17.2 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.24.0 trio-websocket-0.11.1 typing_extensions-4.9.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAHOO!ショッピングのカテゴリーID指定により、その配下のカテゴリーについて、\n",
    "# 上位10位ランキングデータを取得してExcelに登録する。\n",
    "# （ページの構成により、レビュー数／アベレージがズレるケース有り）\n",
    "# Selenium利用により上位30位に拡張可（要コード調整）\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "# import openpyxl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from time import sleep\n",
    "\n",
    "from api import ys_api\n",
    "from exl import exl_wb\n",
    "\n",
    "SEL_USE = True\n",
    "\n",
    "# 入力されたカテゴリーIDに従属するID全てをリスト化\n",
    "ys = ys_api()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        id = input('カテゴリーID')\n",
    "        break\n",
    "    except ValueError:\n",
    "        pass\n",
    "ids = [id]\n",
    "ys.out_ids = []\n",
    "ys.list_get_ids(ids)\n",
    "codes = ys.out_ids\n",
    "\n",
    "del ys\n",
    "\n",
    "if SEL_USE:\n",
    "    # Selenium起動準備\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument('--window-size=3200,1800') \n",
    "    options.add_argument('--user-data-dir=/Users/ynurmj5e/Library/Application Support/Google/Chrome/Default')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Excelファイル出力準備\n",
    "wb = exl_wb('output', 'SpreadSheet.xlsx')\n",
    "if SEL_USE:\n",
    "    wb.open_workbook(30)\n",
    "else:\n",
    "    wb.open_workbook()\n",
    "\n",
    "# メイン処理\n",
    "for code in codes:\n",
    "    # ランキングページ取得\n",
    "    # url = f'https://shopping.yahoo.co.jp/categoryranking/{code}/list?sc_i=shp_pc_cateranking_nrwcgt'\n",
    "    url = f'https://shopping.yahoo.co.jp/categoryranking/{code}/list?sc_i=shopping-pc-web-list-ranking-nrwcgt-slctp_md-ranking'\n",
    "    \n",
    "    if SEL_USE:\n",
    "        # Selenium操作\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, 2500);\")      # 「もっと見る」ボタンが表示されるまでスクロール\n",
    "            sleep(1)\n",
    "            driver.execute_script('arguments[0].click();', driver.find_element(By.CLASS_NAME, \"button-text\"))\n",
    "            sleep(1)\n",
    "        except NoSuchElementException:                              # 「もっと見る」ボタンの無いページスキップ\n",
    "            pass\n",
    "\n",
    "        page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    else:\n",
    "        page = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "    # パンくずリスト取得\n",
    "    categories = page.find_all('li', class_ = 'Breadcrumb__item')\n",
    "    l_cat = []\n",
    "    for category in categories:\n",
    "        l_cat.append(category.text)\n",
    "\n",
    "    # ランキング情報取得\n",
    "    ranks = page.find_all('span', class_ = 'rank-text')\n",
    "    reviews = page.find_all('span', class_ = 'Review__count____2_0_101 Review__count--hasBrackets____2_0_101 review-count')\n",
    "    averages = page.find_all('span', class_ = 'Review__average____2_0_101 review-average')\n",
    "    prices = page.find_all('span', class_ = 'price-number')\n",
    "    names = page.find_all('span', class_ = 'name-text')\n",
    "    images = page.find_all('img', class_ = 'image')\n",
    "\n",
    "    # Excel操作\n",
    "    ws = wb.copy_ws()\n",
    "    ws.title = code\n",
    "    ws['C2'].hyperlink = url\n",
    "\n",
    "    # パンくずリスト登録\n",
    "    for j in range(len(l_cat)):\n",
    "        ws.cell(row=6, column=j + 2, value=l_cat[j])\n",
    "\n",
    "    # ランキング情報登録\n",
    "    for i in range(len(reviews)):\n",
    "        ws.cell(row=i + 9, column=3, value=reviews[i].text)\n",
    "        ws.cell(row=i + 9, column=4, value=float(averages[i].text))\n",
    "        ws.cell(row=i + 9, column=5, value=int(prices[i].text.replace(',', '')))\n",
    "        ws.cell(row=i + 9, column=6, value=names[i].text)\n",
    "        img_url = images[i].get('src')\n",
    "        ws.cell(row=i + 9, column=7).hyperlink = img_url\n",
    "        # ws.cell(row=i + 9, column=8).value = f'=_xlfn.image(G{i + 9})'\n",
    "\n",
    "# Excel終了\n",
    "wb.save_workbook()\n",
    "\n",
    "if SEL_USE:\n",
    "    # Seleniumu終了\n",
    "    driver.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
